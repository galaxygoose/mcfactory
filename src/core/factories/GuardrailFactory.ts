// This file is generated by the Architect Agent.
// Other agents must implement logic INSIDE this file only.
// Do NOT create or delete files. Respect the MIC + MIM.

import {
  Factory,
  GuardrailResult,
  FactoryOptions,
  ProviderRequest,
  ProviderResponse,
  GuardrailConfig
} from '../../types';
import { ProviderRegistry } from '../providers/providerRegistry';

export interface GuardrailInput {
  text: string;
  context?: string;
  userId?: string;
  sessionId?: string;
}

export class GuardrailFactory implements Factory<GuardrailInput, GuardrailResult> {
  private config: GuardrailConfig = {
    enabled: true,
    hallucinationThreshold: 0.7
  };

  constructor(config?: Partial<GuardrailConfig>) {
    this.config = { ...this.config, ...config };
  }

  async run(input: GuardrailInput, options?: FactoryOptions): Promise<GuardrailResult> {
    if (!this.config.enabled) {
      return { safe: true };
    }

    const reasons: string[] = [];

    // Check for banned topics
    if (this.config.bannedTopics) {
      const bannedTopicCheck = this.checkBannedTopics(input.text, this.config.bannedTopics);
      if (!bannedTopicCheck.safe) {
        reasons.push(...bannedTopicCheck.reasons);
      }
    }

    // Check for hallucinations if threshold is set
    if (this.config.hallucinationThreshold && this.config.hallucinationThreshold > 0) {
      const hallucinationCheck = await this.checkHallucinations(input, options);
      if (!hallucinationCheck.safe) {
        reasons.push(...hallucinationCheck.reasons);
      }
    }

    // Additional safety checks
    const safetyCheck = await this.performSafetyChecks(input, options);
    if (!safetyCheck.safe) {
      reasons.push(...safetyCheck.reasons);
    }


    return {
      safe: reasons.length === 0,
      reasons: reasons.length > 0 ? reasons : undefined
    };
  }

  private checkBannedTopics(text: string, bannedTopics: string[]): { safe: boolean; reasons: string[] } {
    const reasons: string[] = [];
    const lowerText = text.toLowerCase();

    for (const topic of bannedTopics) {
      if (lowerText.includes(topic.toLowerCase())) {
        reasons.push(`Content contains banned topic: ${topic}`);
      }
    }

    return {
      safe: reasons.length === 0,
      reasons
    };
  }

  private async checkHallucinations(input: GuardrailInput, options?: FactoryOptions): Promise<{ safe: boolean; reasons: string[] }> {
    const providerName = options?.provider || 'openai';
    const provider = ProviderRegistry.get(providerName);

    if (!provider) {
      // If provider is not available, skip hallucination check
      return { safe: true, reasons: [] };
    }

    const prompt = this.buildHallucinationCheckPrompt(input);

    const request: ProviderRequest = {
      model: 'gpt-4',
      input: prompt,
      options: {
        temperature: 0.1,
        max_tokens: 200,
        ...options?.metadata
      }
    };

    try {
      const response: ProviderResponse = await provider.callModel(request);
      return this.parseHallucinationResponse(response, input);
    } catch (error) {
      console.warn('Hallucination check failed:', error);
      // Don't fail the entire guardrail check if hallucination detection fails
      return { safe: true, reasons: [] };
    }
  }

  private async performSafetyChecks(input: GuardrailInput, options?: FactoryOptions): Promise<{ safe: boolean; reasons: string[] }> {
    const reasons: string[] = [];
    const text = input.text;

    // Check for excessive repetition
    if (this.hasExcessiveRepetition(text)) {
      reasons.push('Content contains excessive repetition');
    }

    // Check for nonsensical content
    if (await this.isNonsensical(text, options)) {
      reasons.push('Content appears to be nonsensical or incoherent');
    }

    // Check for potentially harmful instructions
    if (this.containsHarmfulInstructions(text)) {
      reasons.push('Content contains potentially harmful instructions');
    }

    return {
      safe: reasons.length === 0,
      reasons
    };
  }

  private buildHallucinationCheckPrompt(input: GuardrailInput): string {
    return `Analyze the following text for signs of hallucination, fabrication, or made-up information. Consider if the content is factual, coherent, and grounded in reality.

Text to analyze:
"${input.text}"

${input.context ? `Context: ${input.context}` : ''}

Respond with a JSON object indicating whether the text shows signs of hallucination:
{
  "isHallucination": boolean,
  "confidence": number,
  "explanation": "brief explanation"
}

A hallucination occurs when the content contains:
- Made-up facts or events
- Inconsistent or contradictory information
- Completely fabricated scenarios
- Impossible or unrealistic claims`;
  }

  private parseHallucinationResponse(response: ProviderResponse, input: GuardrailInput): { safe: boolean; reasons: string[] } {
    try {
      const result = typeof response.output === 'string'
        ? JSON.parse(response.output)
        : response.output;

      const isHallucination = result.isHallucination || false;
      const confidence = result.confidence || 0;
      const threshold = this.config.hallucinationThreshold || 0.7;

      if (isHallucination && confidence >= threshold) {
        return {
          safe: false,
          reasons: [`Potential hallucination detected (confidence: ${confidence.toFixed(2)}): ${result.explanation || 'Content may contain fabricated information'}`]
        };
      }

      return { safe: true, reasons: [] };
    } catch (error) {
      // Fallback parsing
      const output = typeof response.output === 'string' ? response.output.toLowerCase() : '';
      if (output.includes('hallucination') || output.includes('fabricated') || output.includes('made up')) {
        return {
          safe: false,
          reasons: ['Potential hallucination detected in content']
        };
      }

      return { safe: true, reasons: [] };
    }
  }

  private hasExcessiveRepetition(text: string): boolean {
    // Simple check for excessive repetition
    const words = text.toLowerCase().split(/\s+/);
    const wordCount = words.length;

    if (wordCount < 10) return false;

    // Count word frequencies
    const wordFreq: Record<string, number> = {};
    words.forEach(word => {
      if (word.length > 3) { // Only check words longer than 3 characters
        wordFreq[word] = (wordFreq[word] || 0) + 1;
      }
    });

    // Check if any word appears more than 20% of the time
    const maxFreq = Math.max(...Object.values(wordFreq));
    return maxFreq > wordCount * 0.2;
  }

  private async isNonsensical(text: string, options?: FactoryOptions): Promise<boolean> {
    // Simple heuristics for nonsensical content
    const issues = [];

    // Check for excessive special characters
    const specialCharRatio = (text.match(/[^a-zA-Z0-9\s]/g) || []).length / text.length;
    if (specialCharRatio > 0.3) {
      issues.push('excessive special characters');
    }

    // Check for very short words ratio (potential gibberish)
    const words = text.split(/\s+/);
    if (words.length > 10) {
      const shortWords = words.filter(word => word.length <= 2).length;
      if (shortWords / words.length > 0.6) {
        issues.push('excessive short words');
      }
    }

    // Check for lack of sentence structure
    const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);
    if (sentences.length > 0) {
      const avgSentenceLength = sentences.reduce((sum, s) => sum + s.length, 0) / sentences.length;
      if (avgSentenceLength < 5) {
        issues.push('very short sentences');
      }
    }

    return issues.length >= 2; // Multiple issues suggest nonsensical content
  }

  private containsHarmfulInstructions(text: string): boolean {
    const harmfulPatterns = [
      /how to (make|build|create).*(bomb|weapon|explosive)/i,
      /instructions? for.*(hacking|phishing|scamming)/i,
      /how to.*(poison|kill|harm)/i,
      /recipe for.*(drugs|narcotics)/i,
      /guide to.*(illegal|criminal)/i
    ];

    return harmfulPatterns.some(pattern => pattern.test(text));
  }
}
