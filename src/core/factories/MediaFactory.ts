// This file is generated by the Architect Agent.
// Other agents must implement logic INSIDE this file only.
// Do NOT create or delete files. Respect the MIC + MIM.

import {
  Factory,
  STTInput,
  STTOutput,
  TTSInput,
  TTSOutput,
  OCRInput,
  OCROutput,
  ImageCaptionInput,
  ImageCaptionOutput,
  FactoryOptions,
  ProviderRequest,
  ProviderResponse
} from '../../types';
import { ProviderRegistry } from '../providers/providerRegistry';

export class MediaFactory {
  async run(input: STTInput, options?: FactoryOptions): Promise<STTOutput>;
  async run(input: TTSInput, options?: FactoryOptions): Promise<TTSOutput>;
  async run(input: OCRInput, options?: FactoryOptions): Promise<OCROutput>;
  async run(input: ImageCaptionInput, options?: FactoryOptions): Promise<ImageCaptionOutput>;
  async run(input: STTInput | TTSInput | OCRInput | ImageCaptionInput, options?: FactoryOptions): Promise<STTOutput | TTSOutput | OCROutput | ImageCaptionOutput> {
    // Determine media task based on input structure
    if (this.isSTTInput(input)) {
      return this.speechToText(input, options);
    } else if (this.isTTSInput(input)) {
      return this.textToSpeech(input, options);
    } else if (this.isOCRInput(input)) {
      return this.opticalCharacterRecognition(input, options);
    } else if (this.isImageCaptionInput(input)) {
      return this.imageCaption(input, options);
    } else {
      throw new Error('Invalid media input type');
    }
  }

  private isSTTInput(input: STTInput | TTSInput | OCRInput | ImageCaptionInput): input is STTInput {
    return 'audioBuffer' in input && input.audioBuffer instanceof ArrayBuffer &&
           !('text' in input) && !('imageBuffer' in input);
  }

  private isTTSInput(input: STTInput | TTSInput | OCRInput | ImageCaptionInput): input is TTSInput {
    return 'text' in input && typeof input.text === 'string';
  }

  private isOCRInput(input: STTInput | TTSInput | OCRInput | ImageCaptionInput): input is OCRInput {
    return 'imageBuffer' in input && input.imageBuffer instanceof ArrayBuffer && (input as any).type === 'ocr';
  }

  private isImageCaptionInput(input: STTInput | TTSInput | OCRInput | ImageCaptionInput): input is ImageCaptionInput {
    return 'imageBuffer' in input && input.imageBuffer instanceof ArrayBuffer && (input as any).type === 'caption';
  }

  private async speechToText(input: STTInput, options?: FactoryOptions): Promise<STTOutput> {
    const providerName = options?.provider || 'whisper';
    const provider = ProviderRegistry.get(providerName);

    if (!provider) {
      throw new Error(`Provider ${providerName} not found for STT`);
    }

    // Validate and preprocess audio
    const processedAudio = await this.preprocessAudio(input.audioBuffer);

    const request: ProviderRequest = {
      model: 'whisper-1', // Default Whisper model
      input: processedAudio,
      options: {
        language: input.language,
        response_format: 'json',
        ...options?.metadata
      }
    };

    const response: ProviderResponse = await provider.callModel(request);

    return this.parseSTTResponse(response, input);
  }

  private async textToSpeech(input: TTSInput, options?: FactoryOptions): Promise<TTSOutput> {
    const providerName = options?.provider || 'elevenlabs';
    const provider = ProviderRegistry.get(providerName);

    if (!provider) {
      throw new Error(`Provider ${providerName} not found for TTS`);
    }

    const request: ProviderRequest = {
      model: 'eleven_monolingual_v1', // Default ElevenLabs model
      input: input.text,
      options: {
        voice: input.voice || 'default',
        stability: 0.5,
        similarity_boost: 0.5,
        ...options?.metadata
      }
    };

    const response: ProviderResponse = await provider.callModel(request);

    return this.parseTTSResponse(response);
  }

  private async opticalCharacterRecognition(input: OCRInput, options?: FactoryOptions): Promise<OCROutput> {
    const providerName = options?.provider || 'openai'; // Using GPT-4 Vision for OCR
    const provider = ProviderRegistry.get(providerName);

    if (!provider) {
      throw new Error(`Provider ${providerName} not found for OCR`);
    }

    // Convert image buffer to base64 for API
    const base64Image = this.bufferToBase64(input.imageBuffer);

    const prompt = 'Extract all text from this image. Provide the text exactly as it appears, preserving formatting and layout where possible. Also provide bounding box coordinates for text blocks if available.';

    const request: ProviderRequest = {
      model: 'gpt-4-vision-preview',
      input: {
        prompt,
        image: base64Image
      },
      options: {
        max_tokens: 1000,
        ...options?.metadata
      }
    };

    const response: ProviderResponse = await provider.callModel(request);

    return this.parseOCRResponse(response);
  }

  private async preprocessAudio(audioBuffer: ArrayBuffer): Promise<ArrayBuffer> {
    // Basic audio preprocessing - simple validation and normalization
    // In a real implementation, this would convert formats, normalize volume, etc.
    try {
      // Simple validation - check if buffer has reasonable size for audio
      if (audioBuffer.byteLength < 1000) {
        throw new Error('Audio buffer too small');
      }
      if (audioBuffer.byteLength > 50 * 1024 * 1024) { // 50MB limit
        throw new Error('Audio buffer too large');
      }
      return audioBuffer;
    } catch (error) {
      // If preprocessing fails, return original buffer
      console.warn('Audio preprocessing failed, using original buffer:', error);
      return audioBuffer;
    }
  }

  private bufferToBase64(buffer: ArrayBuffer): string {
    const bytes = new Uint8Array(buffer);
    let binary = '';
    for (let i = 0; i < bytes.byteLength; i++) {
      binary += String.fromCharCode(bytes[i]);
    }
    return btoa(binary);
  }

  private parseSTTResponse(response: ProviderResponse, input: STTInput): STTOutput {
    try {
      const result = typeof response.output === 'string'
        ? JSON.parse(response.output)
        : response.output;

      return {
        text: result.text || '',
        language: result.language || input.language
      };
    } catch (error) {
      // Fallback for non-JSON responses
      const text = typeof response.output === 'string' ? response.output : '';
      return {
        text: text.trim(),
        language: input.language
      };
    }
  }

  private parseTTSResponse(response: ProviderResponse): TTSOutput {
    // The response output should be the audio buffer
    if (response.output instanceof ArrayBuffer) {
      return {
        audioBuffer: response.output
      };
    }

    // If it's base64 encoded, convert back to ArrayBuffer
    if (typeof response.output === 'string') {
      try {
        const binaryString = atob(response.output);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }
        return {
          audioBuffer: bytes.buffer
        };
      } catch (error) {
        throw new Error('Invalid TTS response format');
      }
    }

    throw new Error('TTS response does not contain valid audio data');
  }

  private parseOCRResponse(response: ProviderResponse): OCROutput {
    try {
      const result = typeof response.output === 'string'
        ? JSON.parse(response.output)
        : response.output;

      return {
        text: result.text || '',
        blocks: Array.isArray(result.blocks) ? result.blocks : undefined
      };
    } catch (error) {
      // Fallback parsing - treat entire response as text
      const text = typeof response.output === 'string' ? response.output : '';
      return {
        text: text.trim()
      };
    }
  }

  private async imageCaption(input: ImageCaptionInput, options?: FactoryOptions): Promise<ImageCaptionOutput> {
    const providerName = options?.provider || 'openai'; // Using GPT-4 Vision for image captioning
    const provider = ProviderRegistry.get(providerName);

    if (!provider) {
      throw new Error(`Provider ${providerName} not found for image captioning`);
    }

    // Convert image buffer to base64 for API
    const base64Image = this.bufferToBase64(input.imageBuffer);

    const prompt = 'Generate a detailed caption for this image. Describe what you see, including objects, people, setting, colors, and any notable features. Be descriptive but concise.';

    const request: ProviderRequest = {
      model: 'gpt-4-vision-preview',
      input: {
        prompt,
        image: base64Image
      },
      options: {
        max_tokens: 150,
        ...options?.metadata
      }
    };

    const response: ProviderResponse = await provider.callModel(request);

    return this.parseImageCaptionResponse(response);
  }

  private parseImageCaptionResponse(response: ProviderResponse): ImageCaptionOutput {
    try {
      const result = typeof response.output === 'string'
        ? JSON.parse(response.output)
        : response.output;

      return {
        caption: result.caption || result.text || '',
        confidence: result.confidence || 0.9
      };
    } catch (error) {
      // Fallback parsing - treat entire response as caption
      const caption = typeof response.output === 'string' ? response.output : '';
      return {
        caption: caption.trim(),
        confidence: 0.8
      };
    }
  }
}
