// This file is generated by the Architect Agent.
// Other agents must implement logic INSIDE this file only.
// Do NOT create or delete files. Respect the MIC + MIM.

import axios, { AxiosError } from 'axios';
import { Provider, ProviderRequest, ProviderResponse } from '../../types';
import { ProviderRegistry } from './providerRegistry';
import { PROVIDER_TYPES } from './providerTypes';

export class LocalLLMProvider implements Provider {
  name = PROVIDER_TYPES.LOCAL;

  async callModel(req: ProviderRequest): Promise<ProviderResponse> {
    const localModelUrl = process.env.LOCAL_MODEL_URL || 'http://localhost:11434';
    const modelPath = process.env.LOCAL_MODEL_PATH;

    if (!modelPath && !localModelUrl) {
      throw new Error('Either LOCAL_MODEL_PATH or LOCAL_MODEL_URL environment variable is required');
    }

    try {
      // Assume Ollama-compatible API for local models
      const response = await axios.post(
        `${localModelUrl}/api/generate`,
        {
          model: req.model,
          prompt: Array.isArray(req.input)
            ? req.input.map(msg => msg.content || msg.message).join('\n')
            : req.input as string,
          stream: false,
          options: req.options || {},
        },
        {
          headers: {
            'Content-Type': 'application/json',
          },
        }
      );

      const data = response.data;
      return {
        output: data.response,
        tokens: data.eval_count,
        model: req.model,
        provider: this.name,
      };
    } catch (error) {
      if (error instanceof AxiosError) {
        throw new Error(`Local LLM API error: ${error.response?.data?.error || error.message}`);
      }
      throw new Error(`Local LLM provider error: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  static create(): Provider {
    const provider = new LocalLLMProvider();
    ProviderRegistry.registerProvider(provider);
    return provider;
  }
}
