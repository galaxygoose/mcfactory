// This file is generated by the Architect Agent.
// Other agents must implement logic INSIDE this file only.
// Do NOT create or delete files. Respect the MIC + MIM.

import { TrainingExample } from '../../types';

/**
 * Parquet format representation for training data
 * Since we don't have a parquet library, this returns a structured representation
 * that can be used with parquet libraries like parquetjs or apache-arrow
 */
export interface ParquetData {
  schema: ParquetSchema;
  rows: Record<string, any>[];
  metadata: {
    created: string;
    rowCount: number;
    compression?: string;
    version: string;
  };
}

export interface ParquetSchema {
  fields: ParquetField[];
}

export interface ParquetField {
  name: string;
  type: ParquetType;
  nullable?: boolean;
  metadata?: Record<string, string>;
}

export type ParquetType =
  | 'BOOLEAN'
  | 'INT32'
  | 'INT64'
  | 'FLOAT'
  | 'DOUBLE'
  | 'BYTE_ARRAY'  // UTF8 strings
  | 'FIXED_LEN_BYTE_ARRAY';

/**
 * Formats training examples into Parquet-compatible data structure
 * This creates the schema and row data that would be written to a parquet file
 */
export function formatParquet(data: TrainingExample[]): ParquetData {
  if (!Array.isArray(data) || data.length === 0) {
    return {
      schema: { fields: [] },
      rows: [],
      metadata: {
        created: new Date().toISOString(),
        rowCount: 0,
        version: '1.0.0'
      }
    };
  }

  // Infer schema from the data
  const schema = inferParquetSchema(data);

  // Convert examples to parquet rows
  const rows = data.map(example => convertToParquetRow(example, schema));

  return {
    schema,
    rows,
    metadata: {
      created: new Date().toISOString(),
      rowCount: data.length,
      compression: 'SNAPPY', // Common parquet compression
      version: '1.0.0'
    }
  };
}

/**
 * Infers a Parquet schema from training examples
 * Analyzes the data structure to determine appropriate column types
 */
function inferParquetSchema(data: TrainingExample[]): ParquetSchema {
  const fields: ParquetField[] = [];

  // Analyze a sample of the data to determine schema
  const sampleSize = Math.min(data.length, 100);
  const sample = data.slice(0, sampleSize);

  // Core fields that are always present
  fields.push(
    {
      name: 'input',
      type: 'BYTE_ARRAY',
      nullable: false,
      metadata: { logicalType: 'UTF8' }
    },
    {
      name: 'output',
      type: 'BYTE_ARRAY',
      nullable: true,
      metadata: { logicalType: 'UTF8' }
    },
    {
      name: 'task',
      type: 'BYTE_ARRAY',
      nullable: false,
      metadata: { logicalType: 'UTF8' }
    }
  );

  // Analyze metadata fields
  const metadataFields = new Map<string, ParquetField>();

  sample.forEach(example => {
    if (example.metadata) {
      analyzeMetadataFields(example.metadata, metadataFields, '');
    }
  });

  // Add metadata fields to schema
  metadataFields.forEach(field => fields.push(field));

  return { fields };
}

/**
 * Recursively analyzes metadata to determine field types
 */
function analyzeMetadataFields(
  metadata: Record<string, any>,
  fields: Map<string, ParquetField>,
  prefix: string
): void {
  for (const [key, value] of Object.entries(metadata)) {
    const fullKey = prefix ? `${prefix}.${key}` : key;

    if (fields.has(fullKey)) continue; // Already analyzed

    const fieldType = inferParquetFieldType(value);
    if (fieldType) {
      fields.set(fullKey, {
        name: fullKey,
        type: fieldType.type,
        nullable: true,
        metadata: fieldType.metadata
      });
    }
  }
}

/**
 * Infers the Parquet type for a given value
 */
function inferParquetFieldType(value: any): { type: ParquetType; metadata?: Record<string, string> } | null {
  if (value === null || value === undefined) {
    return { type: 'BYTE_ARRAY', metadata: { logicalType: 'UTF8' } };
  }

  if (typeof value === 'boolean') {
    return { type: 'BOOLEAN' };
  }

  if (typeof value === 'number') {
    // Use DOUBLE for all numbers to keep it simple
    return { type: 'DOUBLE' };
  }

  if (typeof value === 'string') {
    return { type: 'BYTE_ARRAY', metadata: { logicalType: 'UTF8' } };
  }

  if (Array.isArray(value)) {
    // Convert arrays to JSON strings
    return { type: 'BYTE_ARRAY', metadata: { logicalType: 'UTF8', originalType: 'ARRAY' } };
  }

  if (typeof value === 'object') {
    // Convert objects to JSON strings
    return { type: 'BYTE_ARRAY', metadata: { logicalType: 'UTF8', originalType: 'OBJECT' } };
  }

  // Default to string for unknown types
  return { type: 'BYTE_ARRAY', metadata: { logicalType: 'UTF8' } };
}

/**
 * Converts a training example to a parquet row format
 */
function convertToParquetRow(example: TrainingExample, schema: ParquetSchema): Record<string, any> {
  const row: Record<string, any> = {};

  // Add core fields
  row.input = example.input;
  row.output = example.output || null;
  row.task = example.task;

  // Add metadata fields
  if (example.metadata) {
    flattenMetadataToRow(example.metadata, row, schema);
  }

  // Ensure all schema fields are present in the row (with nulls for missing fields)
  schema.fields.forEach(field => {
    if (!(field.name in row)) {
      row[field.name] = null;
    }
  });

  return row;
}

/**
 * Flattens metadata into the row using the same structure as the schema
 */
function flattenMetadataToRow(
  metadata: Record<string, any>,
  row: Record<string, any>,
  schema: ParquetSchema
): void {
  const schemaFieldNames = new Set(schema.fields.map(f => f.name));

  function flatten(obj: Record<string, any>, prefix: string = ''): void {
    for (const [key, value] of Object.entries(obj)) {
      const fullKey = prefix ? `${prefix}.${key}` : key;

      if (schemaFieldNames.has(fullKey)) {
        if (Array.isArray(value) || (typeof value === 'object' && value !== null)) {
          // Convert complex types to JSON strings as per schema
          row[fullKey] = JSON.stringify(value);
        } else {
          row[fullKey] = value;
        }
      }
    }
  }

  flatten(metadata);
}

/**
 * Converts ParquetData back to TrainingExample array
 * Useful for loading previously formatted data
 */
export function parseParquetData(parquetData: ParquetData): TrainingExample[] {
  const examples: TrainingExample[] = [];

  parquetData.rows.forEach(row => {
    try {
      const example: TrainingExample = {
        input: row.input || '',
        task: row.task || ''
      };

      // Add output if present
      if (row.output !== null && row.output !== undefined) {
        example.output = row.output;
      }

      // Reconstruct metadata
      const metadata: Record<string, any> = {};
      const schemaFieldNames = parquetData.schema.fields.map(f => f.name);

      Object.entries(row).forEach(([key, value]) => {
        if (key !== 'input' && key !== 'output' && key !== 'task' && schemaFieldNames.includes(key)) {
          // Try to parse JSON strings back to original types
          if (typeof value === 'string') {
            try {
              const parsed = JSON.parse(value);
              setNestedValue(metadata, key, parsed);
            } catch {
              // Not valid JSON, keep as string
              setNestedValue(metadata, key, value);
            }
          } else {
            setNestedValue(metadata, key, value);
          }
        }
      });

      if (Object.keys(metadata).length > 0) {
        example.metadata = metadata;
      }

      examples.push(example);
    } catch (error) {
      console.warn(`Failed to parse parquet row: ${error}`);
    }
  });

  return examples;
}

/**
 * Sets a nested value in an object using dot notation
 */
function setNestedValue(obj: Record<string, any>, path: string, value: any): void {
  const keys = path.split('.');
  let current = obj;

  for (let i = 0; i < keys.length - 1; i++) {
    const key = keys[i];
    if (!(key in current) || typeof current[key] !== 'object' || current[key] === null) {
      current[key] = {};
    }
    current = current[key];
  }

  current[keys[keys.length - 1]] = value;
}

/**
 * Validates that the parquet data structure is properly formatted
 */
export function validateParquetData(data: ParquetData): { valid: boolean; errors: string[] } {
  const errors: string[] = [];

  if (!data.schema || !Array.isArray(data.schema.fields)) {
    errors.push('Invalid schema: schema.fields must be an array');
    return { valid: false, errors };
  }

  if (!Array.isArray(data.rows)) {
    errors.push('Invalid data: rows must be an array');
    return { valid: false, errors };
  }

  if (data.rows.length !== data.metadata.rowCount) {
    errors.push(`Row count mismatch: metadata says ${data.metadata.rowCount}, but found ${data.rows.length} rows`);
  }

  // Check that all rows have the required fields
  const requiredFields = data.schema.fields.filter(f => !f.nullable).map(f => f.name);
  const hasInputField = requiredFields.includes('input');
  const hasTaskField = requiredFields.includes('task');

  if (!hasInputField) {
    errors.push('Schema missing required field: input');
  }

  if (!hasTaskField) {
    errors.push('Schema missing required field: task');
  }

  data.rows.forEach((row, index) => {
    if (hasInputField && (!row.input || typeof row.input !== 'string')) {
      errors.push(`Row ${index + 1}: Missing or invalid required field 'input'`);
    }

    if (hasTaskField && (!row.task || typeof row.task !== 'string')) {
      errors.push(`Row ${index + 1}: Missing or invalid required field 'task'`);
    }
  });

  return {
    valid: errors.length === 0,
    errors
  };
}

/**
 * Estimates the size of the parquet data in bytes
 * Useful for determining if sharding is needed
 */
export function estimateParquetSize(data: TrainingExample[]): number {
  if (!Array.isArray(data) || data.length === 0) {
    return 0;
  }

  // Estimate based on the structured data size
  const parquetData = formatParquet(data);
  const jsonString = JSON.stringify(parquetData);

  // Parquet files are typically smaller than JSON due to columnar compression
  // Estimate ~70% of JSON size for compressed parquet
  return Math.ceil(jsonString.length * 0.7);
}
